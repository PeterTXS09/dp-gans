{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dpgan_tf2_by.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qffc0fV8SI9g",
        "outputId": "a76432b2-e40a-48da-af63-60a68ea757b6"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSnevSJbnYvC",
        "outputId": "76456b8b-d134-4f09-b4fa-6867a55012e2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FekA3tYpAAtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c29f0f-983b-4cc2-95c7-21e59649ff0b"
      },
      "source": [
        "%cd /content/drive/MyDrive/Tesis/CreacionDataGAN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Tesis/CreacionDataGAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK4Qu_ljnlNU"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Tesis/CreacionDataGAN\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "klvIPw8wSMUw",
        "outputId": "e3412017-d40a-4c10-d8e1-237287d0b4bb"
      },
      "source": [
        "df = pd.read_csv(\"selected_users_data.csv\")\n",
        "df[\"time-f\"] = pd.to_datetime(df['time']).astype(int)/10**9\n",
        "df = df[['time-f','lat','lon','user']]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time-f</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>user</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.266231e+09</td>\n",
              "      <td>30.174892</td>\n",
              "      <td>121.238187</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.266231e+09</td>\n",
              "      <td>30.174869</td>\n",
              "      <td>121.238065</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.266231e+09</td>\n",
              "      <td>30.175194</td>\n",
              "      <td>121.237918</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.266231e+09</td>\n",
              "      <td>30.175112</td>\n",
              "      <td>121.237957</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.266231e+09</td>\n",
              "      <td>30.175115</td>\n",
              "      <td>121.237967</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         time-f        lat         lon  user\n",
              "0  1.266231e+09  30.174892  121.238187   132\n",
              "1  1.266231e+09  30.174869  121.238065   132\n",
              "2  1.266231e+09  30.175194  121.237918   132\n",
              "3  1.266231e+09  30.175112  121.237957   132\n",
              "4  1.266231e+09  30.175115  121.237967   132"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLGjWZ8hT8VM"
      },
      "source": [
        "class DPGAN:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        random_dim,\n",
        "        discriminatorDims,\n",
        "        generatorDims,\n",
        "        g_optimizer,\n",
        "        d_optimizer,\n",
        "        noise_std,\n",
        "        d_iter,\n",
        "        epsilon\n",
        "    ):\n",
        "\n",
        "        self.noise_std = noise_std\n",
        "        self.d_iter = d_iter\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.random_dim = random_dim\n",
        "        self.discriminatorDims = discriminatorDims\n",
        "        self.generatorDims = generatorDims\n",
        "\n",
        "        self.g_net = self.create_generator()\n",
        "        self.d_net = self.create_discriminator()\n",
        "\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_optimizer = d_optimizer\n",
        "\n",
        "        self.g_loss_store = []\n",
        "        self.d_loss_store = []\n",
        "        self.wdis_store   = []\n",
        "\n",
        "    def create_generator(self):\n",
        "        G = tf.keras.Sequential()\n",
        "        G.add(tf.keras.Input(shape=(self.random_dim,)))\n",
        "        for u in self.generatorDims[:-1]:\n",
        "            G.add(tf.keras.layers.Dense(units=u))\n",
        "            G.add(tf.keras.layers.BatchNormalization(epsilon=self.epsilon))\n",
        "            G.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
        "        G.add(tf.keras.layers.Dense(self.generatorDims[-1], activation=\"relu\"))\n",
        "\n",
        "        return G\n",
        "\n",
        "    def create_discriminator(self):\n",
        "        D = tf.keras.Sequential()\n",
        "        D.add(tf.keras.Input(shape=(self.input_dim,)))\n",
        "        for u in self.discriminatorDims[:-1]:\n",
        "            D.add(tf.keras.layers.Dense(units=u))\n",
        "            D.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
        "        D.add(tf.keras.layers.Dense(self.discriminatorDims[-1], activation=\"relu\"))\n",
        "\n",
        "        return D\n",
        "\n",
        "    def random_noise(self, nrows=None):\n",
        "        return tf.random.uniform(\n",
        "            (self.batch_size if not nrows else nrows, self.random_dim),\n",
        "            minval=-1, maxval=1\n",
        "        )\n",
        "\n",
        "    def dpnoise(self, tensor):\n",
        "        '''add noise to tensor'''\n",
        "        s = tensor.get_shape().as_list()  # get shape of the tensor\n",
        "\n",
        "        rt = tf.random.normal(s, mean=0.0, stddev=self.noise_std)\n",
        "        t = tf.add(tensor, tf.scalar_mul((1.0 / self.batch_size), rt))\n",
        "        return t\n",
        "\n",
        "    @tf.function\n",
        "    def g_train_step(self, x_real):\n",
        "\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            z = self.random_noise()\n",
        "\n",
        "            x_fake = self.g_net(z, training=True)\n",
        "\n",
        "            y_hat_fake = self.d_net(x_fake, training=False)\n",
        "\n",
        "            g_loss = tf.reduce_mean(y_hat_fake)\n",
        "\n",
        "        g_grads = gen_tape.gradient(g_loss, self.g_net.trainable_variables)\n",
        "\n",
        "        self.g_optimizer.apply_gradients(zip(g_grads, self.g_net.trainable_variables))\n",
        "\n",
        "        return g_loss\n",
        "\n",
        "    @tf.function\n",
        "    def d_train_step(self, x_real):\n",
        "\n",
        "        with tf.GradientTape() as disc_tape:\n",
        "            z = self.random_noise()\n",
        "\n",
        "            x_fake = self.g_net(z, training=False)\n",
        "\n",
        "            y_hat_real = self.d_net(x_real, training=True)\n",
        "            y_hat_fake = self.d_net(x_fake, training=True)\n",
        "\n",
        "            d_loss = tf.reduce_mean(y_hat_fake) - tf.reduce_mean(y_hat_real) \n",
        "\n",
        "        d_grads = disc_tape.gradient(d_loss, self.d_net.trainable_variables)\n",
        "\n",
        "        d_grads_new = list(map(self.dpnoise, d_grads))\n",
        "\n",
        "        self.d_optimizer.apply_gradients(zip(d_grads_new, self.d_net.trainable_variables))\n",
        "\n",
        "        self.d_clip = [v.assign(tf.clip_by_value(v, -0.01, 0.01)) for v in self.d_net.trainable_variables]\n",
        "\n",
        "        return d_loss\n",
        "\n",
        "    def train(self, dataset, nepochs, batch_size, output_examples):\n",
        "        self.batch_size = batch_size\n",
        "        self.output_examples = output_examples\n",
        "        with tf.device(\"gpu:0\"):\n",
        "            for epoch in range(nepochs):\n",
        "                start_time = time.time()\n",
        "                pbar = tqdm(dataset, disable=True)\n",
        "                pbar.set_description(\"Epoch {}/{}\".format(epoch+1, nepochs))\n",
        "                d_iter_ = 0\n",
        "                for batch in pbar:\n",
        "                    batch = tf.cast(batch, tf.float32)\n",
        "                    if d_iter_ < self.d_iter:\n",
        "                        rd_loss = self.d_train_step(batch)\n",
        "                        d_iter_ += 1\n",
        "                        continue\n",
        "                    rg_loss = self.g_train_step(batch)\n",
        "                    d_iter_ = 0\n",
        "\n",
        "                    self.g_loss_store.append(rg_loss.numpy())\n",
        "                    self.d_loss_store.append(rd_loss.numpy())\n",
        "\n",
        "        z_sample = self.random_noise(self.output_examples)\n",
        "        x_gene = self.g_net.predict(z_sample)\n",
        "        return x_gene"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK-zeR7Cb-Nh"
      },
      "source": [
        "input_dim = 3\n",
        "random_dim = 100\n",
        "\n",
        "noise_std = [1.5]\n",
        "d_iter = [4]\n",
        "batch_size = [64]\n",
        "optimizer = [\"Adam\", \"RMSProp\"]\n",
        "discriminatorDims = [\n",
        "    [512, 256, 128, 64, 32, 16, 1]\n",
        "]\n",
        "generatorDims = [\n",
        "    [512, 256, 128, 64, input_dim]\n",
        "]\n",
        "users = [1,2,3,5,7,8,9,10,13,14,15,17,21,22,28,32,33,44,49,50,56,61,64,65,66,68,70,77,79,80,90,91,92,101,105,107,115,119,123,127,131,136,137,138,150,152,160,165,169,172]\n",
        "epsilon = [0.001, 0.005, 0.01, 0.1, 1]\n",
        "hyper_pars = []\n",
        "for user in users:\n",
        "  for opt in optimizer:\n",
        "      for d_iter_ in d_iter:\n",
        "          for batch_s in batch_size:\n",
        "              for d_dims in discriminatorDims:\n",
        "                  for g_dims in generatorDims:\n",
        "                      for noise in noise_std:\n",
        "                          for eps in epsilon:\n",
        "                            hyper_pars.append({\n",
        "                              \"user\": user,\n",
        "                              \"input_dim\": input_dim,\n",
        "                              \"random_dim\": random_dim,\n",
        "                              \"optimizer\": opt,\n",
        "                              \"d_iter\": d_iter_,\n",
        "                              \"noise_std\": noise,\n",
        "                              \"batch_size\": batch_s,\n",
        "                              \"discriminatorDims\": d_dims,\n",
        "                              \"generatorDims\": g_dims,\n",
        "                              \"epsilon\": eps\n",
        "                          })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP-iMAEBdZC6"
      },
      "source": [
        "nepochs = 1000\n",
        "for param in hyper_pars:\n",
        "    \n",
        "    dir_name = os.path.join(\n",
        "        path, \"outputs_dpgan\", \"user_{}\".format(param[\"user\"])\n",
        "    )\n",
        "    filename = \"data_{}_batchSize_{}_noise_std_{}_dIter_{}_dDims_{}_gDims_{}_eps_{}.csv\".format(\n",
        "        param[\"optimizer\"],\n",
        "        param[\"batch_size\"],\n",
        "        param[\"noise_std\"],\n",
        "        param[\"d_iter\"],\n",
        "        param[\"epsilon\"]\n",
        "        \"_\".join(map(str, param[\"discriminatorDims\"])),\n",
        "        \"_\".join(map(str, param[\"generatorDims\"]))\n",
        "    )\n",
        "    \n",
        "    if not os.path.isfile(os.path.join(dir_name, filename)):\n",
        "      if param[\"optimizer\"] == \"Adam\":\n",
        "          g_optimizer = tf.keras.optimizers.Adam(\n",
        "              learning_rate=1e-4, \n",
        "              beta_1=0.5, \n",
        "              beta_2=0.9\n",
        "          )\n",
        "          d_optimizer = tf.keras.optimizers.Adam(\n",
        "              learning_rate=1e-4, \n",
        "              beta_1=0.5, \n",
        "              beta_2=0.9\n",
        "          )\n",
        "      else:\n",
        "          g_optimizer = tf.keras.optimizers.RMSprop(learning_rate=2.5e-5)\n",
        "          d_optimizer = tf.keras.optimizers.RMSprop(learning_rate=2.5e-5)\n",
        "          \n",
        "      dp = DPGAN(\n",
        "          param[\"input_dim\"],\n",
        "          param[\"random_dim\"],\n",
        "          param[\"discriminatorDims\"],\n",
        "          param[\"generatorDims\"],\n",
        "          g_optimizer,\n",
        "          d_optimizer,\n",
        "          param[\"noise_std\"],\n",
        "          param[\"d_iter\"],\n",
        "          param[\"epsilon\"]\n",
        "      )\n",
        "      \n",
        "      data = df[df.user == param[\"user\"]][['time-f','lat','lon']]\n",
        "\n",
        "      scaler = StandardScaler()\n",
        "      scaler.fit(data)\n",
        "      data = scaler.transform(data)\n",
        "\n",
        "      dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(50000).batch(param[\"batch_size\"], drop_remainder=True)\n",
        "      \n",
        "      results = dp.train(dataset, nepochs, param[\"batch_size\"], data.shape[0])\n",
        "\n",
        "      if not os.path.isdir(dir_name):\n",
        "          os.makedirs(dir_name)\n",
        "\n",
        "      temp = scaler.inverse_transform(results)\n",
        "      temp = pd.DataFrame(temp)\n",
        "      temp.rename(columns={1:'lat',\n",
        "                            2:'lon',\n",
        "                            0:'time'}, inplace=True)\n",
        "\n",
        "      temp['time'] = pd.to_datetime(temp['time'], unit='s')\n",
        "      temp['user'] = param[\"user\"]\n",
        "      temp = temp[['user','time','lat','lon']]\n",
        "      temp.to_csv(os.path.join(dir_name, filename), index=False, header=False)\n",
        "      del temp, results\n",
        "    else:\n",
        "      print(\"skipped %s\" % filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN_Z0S42k8L0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}